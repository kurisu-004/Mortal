{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module libriichi:\n",
      "\n",
      "NAME\n",
      "    libriichi\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides implementations of the riichi mahjong including the\n",
      "    following features:\n",
      "\n",
      "    - The core feature - player state maintenance driven by mjai events (via\n",
      "      `state.PlayerState`).\n",
      "    - Read mjai logs and produce a batch of instances for training (via\n",
      "      `dataset`).\n",
      "    - Self-play under standard Tenhou rules (via `arena`).\n",
      "    - Definitions of observation and action space for Mortal (via `consts`).\n",
      "    - Statistical works on mjai logs (via `stat.Stat`).\n",
      "    - mjai interface (via `mjai.Bot`).\n",
      "\n",
      "DATA\n",
      "    __all__ = ['__profile__', '__version__', 'consts', 'state', 'dataset',...\n",
      "    __profile__ = 'release'\n",
      "\n",
      "VERSION\n",
      "    0.1.0\n",
      "\n",
      "FILE\n",
      "    e:\\code\\mortal\\mortal\\libriichi.pyd\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libriichi\n",
    "help(libriichi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = libriichi.dataset.GameplayLoader(version = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libriichi  # 假设你的 Rust 模块名为 libriichi\n",
    "import logging\n",
    "import gzip\n",
    "\n",
    "def load_and_process_gameplay(rust_loader, mjai_log_path):\n",
    "    \"\"\"\n",
    "    加载并处理单个 mjai 日志文件的 Gameplay 数据。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with gzip.open(mjai_log_path, \"rt\") as f:\n",
    "            log_content = f.read()\n",
    "        gameplays = rust_loader.load_log(log_content)\n",
    "        logging.info(f\"Successfully loaded {len(gameplays)} Gameplay instances from {mjai_log_path}\")\n",
    "        return gameplays\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading log {mjai_log_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 示例：初始化 Rust 的 GameplayLoader\n",
    "loaders = []\n",
    "for version in [1, 2, 3, 4]:\n",
    "    loader = libriichi.dataset.GameplayLoader(version=version, augmented = False)\n",
    "    loaders.append(loader)\n",
    "\n",
    "# 示例：加载并处理一个转换后的 mjai 日志文件\n",
    "mjai_log_path = \"./dataset/train/20230101.html/2023010100gm-00a9-0000-7fcb3f13.json.gz\"\n",
    "gameplays_v = []\n",
    "for loader in loaders:\n",
    "    gameplays = load_and_process_gameplay(loader, mjai_log_path)\n",
    "    if gameplays is not None:\n",
    "        gameplays_v.append(gameplays)\n",
    "\n",
    "gameplay = gameplays_v[0][0]\n",
    "obs = gameplay.take_obs()\n",
    "invisible_obs = gameplay.take_invisible_obs()\n",
    "actions = gameplay.take_actions()\n",
    "at_kyoku = gameplay.take_at_kyoku()\n",
    "dones = gameplay.take_dones()\n",
    "apply_gamma = gameplay.take_apply_gamma()\n",
    "at_turns = gameplay.take_at_turns()\n",
    "grp = gameplay.take_grp()\n",
    "player_id = gameplay.take_player_id()\n",
    "shantens = gameplay.take_shantens()\n",
    "masks = gameplay.take_masks()\n",
    "\n",
    "# grp\n",
    "feature = grp.take_feature()\n",
    "rank_by_player = grp.take_rank_by_player()\n",
    "final_scores = grp.take_final_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手牌\n",
      "[[0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "aka:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "shanten:\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "wait:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "furiten:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "player_id = 0\n",
    "move = 0\n",
    "print(\"手牌\")\n",
    "print(invisible_obs[move][player_id*15:player_id*15+4])\n",
    "print(\"aka:\")\n",
    "print(invisible_obs[move][player_id*15+4:player_id*15+7])\n",
    "print(\"shanten:\")\n",
    "print(invisible_obs[move][player_id*15+7:player_id*15+13])\n",
    "print(\"wait:\")\n",
    "print(invisible_obs[move][player_id*15+13])\n",
    "print(\"furiten:\")\n",
    "print(invisible_obs[move][player_id*15+14])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss = []\n",
    "for gameplays in gameplays_v:\n",
    "    for gameplay in gameplays:\n",
    "        obss.append(gameplay.take_obs())\n",
    "gameplays = gameplays_v[0]\n",
    "actions = []\n",
    "for gameplay in gameplays:\n",
    "    actions.append(gameplay.take_actions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dora指示牌编码:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "牌河的编码以四个通道一组，分别表示杠牌，舍牌，红宝牌和普通dora牌\n",
      "自家牌河编码(前6):\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "--------------------------------------------------------------------\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "--------------------------------------------------------------------\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "--------------------------------------------------------------------\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "--------------------------------------------------------------------\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "--------------------------------------------------------------------\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "version = 1\n",
    "player_id = 1\n",
    "decision_points_idx = 39\n",
    "# print(\"obs len:\", len(obss[(version-1)*4+player_id]))\n",
    "# print(\"手牌编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][0:7])\n",
    "# print(\"点数编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][7:11])\n",
    "# print(\"排名编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][11:15])\n",
    "# print(\"局数编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][15:19])\n",
    "# print(\"本场和供托编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][19:39])\n",
    "# print(\"场风与自风编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][39:41])\n",
    "print(\"dora指示牌编码:\\n\", obss[(version-1)*4+player_id][decision_points_idx][41:48])\n",
    "print(\"牌河的编码以四个通道一组，分别表示杠牌，舍牌，红宝牌和普通dora牌\")\n",
    "print(\"自家牌河编码(前6):\\n\")\n",
    "for i in range(6):\n",
    "    print(obss[(version-1)*4+player_id][decision_points_idx][48+i*4:48+i*4+4])\n",
    "    print(\"--------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions length: 69\n",
      "player0:\n",
      " [9, 29, 0, 8, 31, 15, 12, 45, 11, 20, 27, 45, 23, 17, 37, 3, 45, 28, 31, 27, 33, 2, 27, 19, 45, 20, 45, 3, 3, 18, 27, 30, 27, 33, 45, 10, 33, 32, 27, 26, 31, 28, 5, 29, 0, 45, 15, 20, 2, 18, 40, 3, 32, 15, 24, 0, 8, 31, 9, 18, 45, 20, 8, 45, 37, 24, 21, 24, 43]\n"
     ]
    }
   ],
   "source": [
    "print(\"actions length:\", len(actions[0]))\n",
    "print(\"player0:\\n\", actions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成用于训练grp模型的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list length: 4142\n"
     ]
    }
   ],
   "source": [
    "from libriichi.dataset import Grp\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "buffer = []\n",
    "file_list = glob.glob(\"dataset/train/**/*.json.gz\", recursive=True)\n",
    "print(\"file_list length:\", len(file_list))\n",
    "data = Grp.load_gz_log_files(file_list[0:10])\n",
    "for game in data:\n",
    "    feature = game.take_feature()\n",
    "    rank_by_player = game.take_rank_by_player()\n",
    "\n",
    "    for i in range(feature.shape[0]):\n",
    "        inputs_seq = torch.as_tensor(feature[:i + 1], dtype=torch.float64)\n",
    "        buffer.append((\n",
    "            inputs_seq,\n",
    "            rank_by_player,\n",
    "        ))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer length: 12\n",
      "buffer[0]: (tensor([[0.0000, 0.0000, 0.0000, 2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "       dtype=torch.float64), [3, 0, 1, 2])\n",
      "buffer[1]: (tensor([[0.0000, 0.0000, 0.0000, 2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [1.0000, 0.0000, 0.0000, 2.5000, 2.6000, 2.5000, 2.4000]],\n",
      "       dtype=torch.float64), [3, 0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"buffer length:\", len(buffer))\n",
    "print(\"buffer[0]:\", buffer[0])\n",
    "print(\"buffer[1]:\", buffer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_seq: tensor([[0.0000, 0.0000, 0.0000, 2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [1.0000, 0.0000, 0.0000, 2.5000, 2.1100, 2.4000, 2.9900],\n",
      "        [2.0000, 0.0000, 0.0000, 2.3000, 1.7200, 2.2000, 3.7800],\n",
      "        [2.0000, 1.0000, 0.0000, 2.2500, 1.6700, 2.3500, 3.7300],\n",
      "        [2.0000, 2.0000, 0.0000, 1.8500, 1.2700, 3.5500, 3.3300],\n",
      "        [3.0000, 0.0000, 0.0000, 1.5300, 1.5900, 3.5500, 3.3300],\n",
      "        [4.0000, 0.0000, 0.0000, 2.3300, 1.5900, 2.7500, 3.3300],\n",
      "        [4.0000, 1.0000, 0.0000, 3.6000, 1.1000, 2.3600, 2.9400]],\n",
      "       dtype=torch.float64)\n",
      "rank_by_player: [1, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "from libriichi.dataset import Grp\n",
    "from train_grp import GrpFileDatasetsIter\n",
    "import torch\n",
    "\n",
    "file_list = glob.glob(\"dataset/train/**/*.json.gz\", recursive=True)\n",
    "dataloader = GrpFileDatasetsIter(file_list, file_batch_size=50, cycle=False)\n",
    "for i, (inputs_seq, rank_by_player) in enumerate(dataloader):\n",
    "    print(\"inputs_seq:\", inputs_seq)\n",
    "    print(\"rank_by_player:\", rank_by_player)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证train.py的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     59\u001b[0m max_grad_norm \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 61\u001b[0m mortal \u001b[38;5;241m=\u001b[39m \u001b[43mBrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(version\u001b[38;5;241m=\u001b[39mversion)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m aux_net \u001b[38;5;241m=\u001b[39m AuxNet((\u001b[38;5;241m4\u001b[39m,))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\RL\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import prelude\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "from os import path\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from torch import optim, nn\n",
    "from torch.amp import GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from common import submit_param, parameter_count, drain, filtered_trimmed_lines, tqdm\n",
    "from player import TestPlayer\n",
    "from dataloader import FileDatasetsIter, worker_init_fn\n",
    "from lr_scheduler import LinearWarmUpCosineAnnealingLR\n",
    "from model import Brain, DQN, AuxNet\n",
    "from libriichi.consts import obs_shape\n",
    "from config import config\n",
    "\n",
    "version = config['control']['version']\n",
    "\n",
    "online = config['control']['online']\n",
    "batch_size = config['control']['batch_size']\n",
    "opt_step_every = config['control']['opt_step_every']\n",
    "save_every = config['control']['save_every']\n",
    "test_every = config['control']['test_every']\n",
    "submit_every = config['control']['submit_every']\n",
    "test_games = config['test_play']['games']\n",
    "min_q_weight = config['cql']['min_q_weight']\n",
    "next_rank_weight = config['aux']['next_rank_weight']\n",
    "assert save_every % opt_step_every == 0\n",
    "assert test_every % save_every == 0\n",
    "\n",
    "device = torch.device(config['control']['device'])\n",
    "torch.backends.cudnn.benchmark = config['control']['enable_cudnn_benchmark']\n",
    "enable_amp = config['control']['enable_amp']\n",
    "enable_compile = config['control']['enable_compile']\n",
    "\n",
    "pts = config['env']['pts']\n",
    "gamma = config['env']['gamma']\n",
    "file_batch_size = config['dataset']['file_batch_size']\n",
    "reserve_ratio = config['dataset']['reserve_ratio']\n",
    "num_workers = config['dataset']['num_workers']\n",
    "num_epochs = config['dataset']['num_epochs']\n",
    "enable_augmentation = config['dataset']['enable_augmentation']\n",
    "augmented_first = config['dataset']['augmented_first']\n",
    "eps = config['optim']['eps']\n",
    "betas = config['optim']['betas']\n",
    "weight_decay = config['optim']['weight_decay']\n",
    "max_grad_norm = config['optim']['max_grad_norm']\n",
    "\n",
    "mortal = Brain(version=version, **config['resnet']).to(device)\n",
    "dqn = DQN(version=version).to(device)\n",
    "aux_net = AuxNet((4,)).to(device)\n",
    "all_models = (mortal, dqn, aux_net)\n",
    "if enable_compile:\n",
    "    for m in all_models:\n",
    "        m.compile()\n",
    "\n",
    "logging.info(f'version: {version}')\n",
    "logging.info(f'obs shape: {obs_shape(version)}')\n",
    "logging.info(f'mortal params: {parameter_count(mortal):,}')\n",
    "logging.info(f'dqn params: {parameter_count(dqn):,}')\n",
    "logging.info(f'aux params: {parameter_count(aux_net):,}')\n",
    "\n",
    "mortal.freeze_bn(config['freeze_bn']['mortal'])\n",
    "\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "for model in all_models:\n",
    "    params_dict = {}\n",
    "    to_decay = set()\n",
    "    for mod_name, mod in model.named_modules():\n",
    "        for name, param in mod.named_parameters(prefix=mod_name, recurse=False):\n",
    "            params_dict[name] = param\n",
    "            if isinstance(mod, (nn.Linear, nn.Conv1d)) and name.endswith('weight'):\n",
    "                to_decay.add(name)\n",
    "    decay_params.extend(params_dict[name] for name in sorted(to_decay))\n",
    "    no_decay_params.extend(params_dict[name] for name in sorted(params_dict.keys() - to_decay))\n",
    "param_groups = [\n",
    "    {'params': decay_params, 'weight_decay': weight_decay},\n",
    "    {'params': no_decay_params},\n",
    "]\n",
    "optimizer = optim.AdamW(param_groups, lr=1, weight_decay=0, betas=betas, eps=eps)\n",
    "scheduler = LinearWarmUpCosineAnnealingLR(optimizer, **config['optim']['scheduler'])\n",
    "scaler = GradScaler(device.type, enabled=enable_amp)\n",
    "test_player = TestPlayer()\n",
    "best_perf = {\n",
    "    'avg_rank': 4.,\n",
    "    'avg_pt': -135.,\n",
    "}\n",
    "\n",
    "steps = 0\n",
    "state_file = config['control']['state_file']\n",
    "best_state_file = config['control']['best_state_file']\n",
    "if path.exists(state_file):\n",
    "    state = torch.load(state_file, weights_only=True, map_location=device)\n",
    "    timestamp = datetime.fromtimestamp(state['timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    logging.info(f'loaded: {timestamp}')\n",
    "    mortal.load_state_dict(state['mortal'])\n",
    "    dqn.load_state_dict(state['current_dqn'])\n",
    "    aux_net.load_state_dict(state['aux_net'])\n",
    "    if not online or state['config']['control']['online']:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "    scaler.load_state_dict(state['scaler'])\n",
    "    best_perf = state['best_perf']\n",
    "    steps = state['steps']\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "mse = nn.MSELoss()\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    logging.info(f'device: {device} ({torch.cuda.get_device_name(device)})')\n",
    "else:\n",
    "    logging.info(f'device: {device}')\n",
    "\n",
    "if online:\n",
    "    submit_param(mortal, dqn, is_idle=True)\n",
    "    logging.info('param has been submitted')\n",
    "\n",
    "writer = SummaryWriter(config['control']['tensorboard_dir'])\n",
    "stats = {\n",
    "    'dqn_loss': 0,\n",
    "    'cql_loss': 0,\n",
    "    'next_rank_loss': 0,\n",
    "}\n",
    "all_q = torch.zeros((save_every, batch_size), device=device, dtype=torch.float32)\n",
    "all_q_target = torch.zeros((save_every, batch_size), device=device, dtype=torch.float32)\n",
    "idx = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
